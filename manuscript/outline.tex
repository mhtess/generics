%todo:

% - if time: look at "order effect" in prior elicitation (taboo sampling hypothesis... first sample is 0, second is 100, third - fifth are meh)


% - try rationality parameter in tfbt model

% - look at "order effect" in prior elicitation (taboo sampling hypothesis... first sample is 0, second is 100, third - fifth are meh)		

%todo after cogsci:

%-run additional contexts. e.g. separate dangerous and distinct.
%-look again at finer-grained truth judgement task?
%-think about the weird cases of generics in the literature. e.g. ``robins lay eggs''.
% - try an S2 model of prevalence judgement task, too. does it reduce to the L1 model?

% -- prior elicitation for accidental / disease states
% -- asymmetry weakened

% -- most / some: better experiments
% -- asymmetry X prior analysis



\documentclass{article}
\usepackage{outlines} 
\setlength{\topmargin}{-.5in} 
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in} 
\setlength{\textwidth}{6in}

 \begin{document}

\title{Words are vague}
\author{MHT, NDG} \maketitle
 
\begin{outline}
  \0 Manuscript outline
  
  Start with cogsci paper.
  
  		\1 Introduction : The semantic puzzle of generics. Use examples to show how the truth conditions can have wide variability. Explain logic of the approach: formal models, underspecified semantics (priors lead to meaning), asymmetry from dependent measures / QUDs
			\2 Good example (sorry Obama)
			\2 Identification problem vs. meaning problem? 
				\3 ID problem: Cimpian \& Markman (2008), Crone \& Frank (2015)
			\2 Generics are ubiquitous and early emerging: Gelman \& Pappas (1998), other early Gelman paper (free play)
			\2 Children's understanding of generics matches adult understanding earlier than for quantifiers: Brandone et al. (2014)
			\2 Flexible truth conditions and powerful implications: Cimpian, Brandone and Gelman (2010)
	
\vspace{2cm}
	\0 Work (done, or to do)
		\1 Prior experiments
			\2 DD / NN / Plain
				\3 5 free response (n =100; trials = 6)
					\4 Trimodal with peaks at 0, 20, 100
					\4 Means, medians and ``overall distributions'' all different
					\4$ \mu_{NN} > \mu_{Plain} > \mu_{DD} $
				\3 1 free response (n = 30; trials = 30)
					\4 DD: Peaks at 0-10; 50; 100
					\4 NN: Peaks at 10, 25, 50, 100
					\4 Plain: Peaks at 0-10; 20-30; 50; 100
				\3 Binned histograms (n = 30; trials = 4)
					\4 Much more mushy
			\2 D / D / Plain \textbf{(not yet analyzed)}
				\3 5 free response (n = 30; trials = 6)
				\3 1 free response (n = 30; trials = 30)
			\2 General dependent measure alternatives
				\3 Sliders
					\4 Single slider
					\4 5 sliders
				\3 Training phase
					\4 Here is a sentence and property and such.
					\4 Here how you would rate this sentence using this scale.
					\4 e.g., None of the X have it ---2---3--- Half ---5---6--- All.
				\3 Replace ``exact number'' dependent measure with slider? (for priors and implied prevalence)
				\3 2 stage response
					\4 ``Do you think some dorbs have dangerous red scales?''
					\4 If so, ``What percentage of dorbs have dangerous red scales?'' / other DM (see below)
					
		\1 Generics experiments
			\2 BGH picture paradigm
				\3 Benefits
					\4 Less pragmatically awkward in evidence... (``10\% have purple feathers'')
					\4 Response format can be drawing (``paint how many lorches you think have purple feathers'')
				\3 Issues
					\4 Left to judgment that the category refers to the animals people are seeing (can be alleviated by just telling them... ``These are Xs'')
					\4 Also left to judgment that this sample is representative of the whole (relies on ``representativeness heuristic'') [Of course, it's a pragmatic inference that this sample is representative, given that the alien is trying to teach people about the animals...]
					\4 What is the base case? (How to display not purple feathers? ``White'' or ``other colors''?)
					\4 Can we do dangerous and distinct in this paradigm?
			\2 Further experiments (CBG-style; or BGH-style if the ``context'' sentences work with the pictures)
				\3 Replicate ``accidental'' properties ?
				\3 Irrelevant / Distinct / Plain condition: If ``irrelevant'' showed the same effect as ``dangerous'' in the D / D / Plain version
					\4 ``Context'' manipulation manipulation more than just ``concept of dangerous''?
					\4  Some sort of (pragmatic) conjunction fallacy? ``more premises $\rightarrow$ more likely to be true?''
					\4 If works: model pragmatics of saying more things?
		\1 Modeling
			\2 Do Justine-style ``plug in prior'' forward predictions
				\3 Backing out the prior assuming $\beta$ could be problematic
					\4 the strong endorsement at low prevalence levels comes from a bump at low prevalence levels
				\3 The prior peak at 20 could give us this small bump $\rightarrow$ increase in endorsements (better match to data)
			\2 Clean up how we're modeling the two judgment tasks? Both listeners?
				\3 Role of alternatives?
				\3 Role of QUD?
				
					
					
  
%		\1 Experiment 1 a \& b: Replicate CBG \emph{truth conditions} \& \emph{implied prevalence}
%			\2 standard freq. stats
%		\1 Bayesian Data Analysis of fixed-threshold semantics
%			\2  Assume generic had a fixed-threshold, that varied by context alone. What is that threshold? 
%				\3 \emph{truth conditions} would be S1 model
%				\3 \emph{implied prevalence} would be L0 model?
%			\2 \emph{Bayesian Data Analysis}, conditioning on both data sets
%				\3 Posterior predictive of fixed-threshold model
%				\3 Posterior predictive pretty bad.
%				\3 Guessing $\phi$ parameter pretty high.
%				\3 Asymmetry?
%		\1 BDA of Lifted-variable RSA
%			\2 Bayesian Language-Understanding Model: lifted-variable model of generics
%			\2 Context inferred in language model... what differs is the prior
%				\3 \emph{truth conditions} would be S2
%				\3 \emph{implied prevalence} would be L1
%			\2 Condition on Exp 1 data to infer hyperprior parameters: $\gamma$ and $\delta$ 
%				\3 Posterior predictive of lvRSA super good
%				\3 Guessing $\phi$ parameter reasonable
%				\3 Asymmetry
%			\2  strong predictions about what priors should look like
%		\1 Experiment 2: Prior elicitation
%			\2 Show qualitative fit 
%		\1 Further simulations using empirical or inferred priors
%			\2 Symmetry / asymmetry with quantifiers most and some
%			\2 Reduced asymmetry with alternative priors (e.g. accidental / disease states)
%				
							
 \end{outline}


%\vspace{2cm}
%A slight alternative could be to go through explaining lvRSA and then do Experiment 2. If this model is the correct model, then priors should vary. So elicit prior. Then do TFBT with just $\phi$, and posterior predictives and asymmetry as before.
%	
	
% 		\2 Evidence required for accepting a generic is different from that which the generic implies
%	\3 \emph{Bayesian Language-Understanding Model: RSA with QUDs and roles in communication}
%	\3 This asymmetry was not present for the quantifier ``most''
%	\3 Degen and Goodman (2014 cogsci) found evidence that different dependent measures may map onto different communicative roles, formalized by speakers and listeners in RSA
%	\3 Q: Can the asymmetry of the generic (and the symmetry of ``most'') be explained by speaker and listener in RSA?
%	\3 Experiment: replicate CBG experiments 1 \& 2 (and be explicit about the analysis of ``some'', which CBG was not)
%	\3 Model: RSA with QUDs and speaker/listener for the 2 tasks
%		\4 \emph{Truth conditions} task $\rightarrow$ QUD = ``quantifier is true?'', task is Speaker (S1 or S2)
%		\4 \emph{Implied prevalence} task $\rightarrow$ QUD = ``how many?'', task is Listener (L1)
%	\3 Include prior elicitation experiment?

			
 \end{document}


