
%%
% bayesian data analyses
% expt1a
%	- prevalence for categories (e..g "lions" that "have manes"):  
%          - phi (uniform 0 1)
%         -  Beta(gamma, delta)  --- gamma (uniform 0 1); delta  (uniform 0 50)
% 		- using posterior on gamma for model
%   - prevalence prior for properties (e.g. "have manes"): 
%		- theta_across (uniform 0 1)	
%          - phi (uniform 0 1)
%         -  Beta(gamma_within, delta_within)  --- gamma (uniform 0 1); delta  (uniform 0 50)
% 		-  using posterior on beta for model
% expt1b
%   - speaker optimality
%	- phi
% expt 2a
%  - Beta_across (gamma_across, delta_across) - 
%  - Beta_within (gamma_within, delta_within) -
%  - phi
% expt 2b
%   - speaker optimality (truth conditions)
%   - speaker optimality (implied prevalence)
%	- phi





% mh 7/19
% in experiment 1 and model, take the speaker perspective when talking about how the model works

%todo after cogsci:
%-run additional contexts. e.g. separate dangerous and distinct.
%-look again at finer-grained truth judgement task?
%-think about the weird cases of generics in the literature. e.g. ``robins lay eggs''.
% - try an S2 model of prevalence judgement task, too. does it reduce to the L1 model?

% comments from workshop class 4/29
%
% why spend a lot of time in replication?
% 7 --- summary sentence 2nd paragraph to intro
% abstract worrisome
% look at study discussions -- move stuff to intro
%
% Lay eggs vs. Are female /// John is tall vs. ESB is tall [move even earlier]
% Move "a natural foil" earlier
%
% Maybe add talk of stereotypes?
% Craig: Use speaker / listener actors more; think about the person and the thing they're doing
% %
%  beef up penultimate paragraph in intro: talk about why distinctive and dangerous are important
% ---- unpack distinctive and dangerous (use experimental materials)
%
% TABLE for materials and experiment. 
% maybe set up replication-idea early.  and then go on.




% details
%
% Section 4: exp 2 intro
%% oceans underneath each sentence
%% connect back with behavior--- asymmetry, truth cond
%
% pre Exp 3: use 
% "endemic to the entire category"
% exp3a: jargon word 
% (reuse penultimate intro paragraph throughout) [maybe make higher level version of this paragraph for intro]
%
% pre-experiments, "Like Exp 2c, 3b did BLAH but differed "
%
% add more to figure captions?
%
%
% seeds of very last paragraph in earlier
%
% in conclusion: what do the tools allow us to do? (e.g. w/ stereotyping) 
% differences in prior knowledge --> how does this contribute to language change?
% 

\documentclass[10pt,letterpaper]{article}

\usepackage{setspace}
\doublespacing
\usepackage{geometry}
\geometry{legalpaper, margin=1in}

\usepackage{pslatex}
\usepackage{apacite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{lipsum}


\graphicspath{{figures/}}

\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip2em
  \hbox{}\nobreak\hfil(#1)%
  \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
  {\savebox\mybox{#1}\begin{quote}}
  {\signed{\usebox\mybox}\end{quote}}


 \newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}

\definecolor{Red}{RGB}{255,0,0}
\newcommand{\red}[1]{\textcolor{Red}{#1}}  

\usepackage{titlesec}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}



\title{Generic language is vague yet rationally / pragmatically understood}

\author{{\large \bf Michael Henry Tessler} (mtessler@stanford.edu)\\ {\large \bf Noah D. Goodman} (ngoodman@stanford.edu) \\
  Department of Psychology, Stanford University}
 
\begin{document}

\maketitle


\begin{abstract}
Generic utterances (e.g. ``Dogs bark'') express ideas about categories in the world and are ubiquitous in everyday conversation. 
Despite their prevalence, the meanings of generic statements are puzzling to formal theories. 
It is believed that generic utterances behave somewhat like quantified ones (e.g. ``Most dogs bark.''). 
If this is correct, however, there should be some threshold on the percentage of the category that must display the property (e.g. some criterion number of dogs that bark) for the utterance to be true.
This threshold is notoriously difficult to define for generics, which are flexible enough to be true based on very little evidence while also implying near-universality of the property. 
Here, we formalize a semantics and pragmatics for generic language understanding, based on the idea that generics are vague: The threshold for acceptance is represented as an unknown property of the language and is actively reasoned about in context by a listener who assumes she is in conversation with an informative speaker. 
We explain how this simple semantics can account for two central phenomena surrounding generic language: The flexible conditions by which a generic is true and the variable implications of generic language. 
Inferences from the model are driven largely by the listeners' \emph{a priori} beliefs about the property, which we measure empirically.
This work suggests that the semantics of generic statements can treated as scalar and that listeners' expectations in the form of prior beliefs heavily govern generic interpretation. 


\textbf{Keywords:} 
generics; semantics; probabilisitic pragmatics
\end{abstract}

Imagine talking with a 2-year-old about colors.
Colors are difficult to describe because they take shape in many different physical forms and the function of color is abstract.
You might offer the toddler something generic like, ``Apples are red.''. 
Few would argue with the truth of this sentence, yet its precise meaning is difficult to specify. 

This type of utterance is generic \cite{Carlson1977, Leslie2008} in that it conveys a generalization about a category (e.g. \emph{shoes}).  
Generics are one of the simplest linguistic constructs and are ubiquitous in everyday conversation, child-directed speech \cite{Gelman2008}, and are the primary vessel by which speakers discuss social categories, making them central to communicating stereotypes \cite{GelmanEtAl2004, Cimpian2010motivation, Leslie2015}.
They have no explicitly marked operator and yet children as young as two or three correctly understand that they refer to categories and not just a plurality in a scene \cite{Cimpian2008}. 
They are believed to be essential to the growth of conceptual knowledge \cite{Gelman2004} and how we represent kinds \cite{Leslie2008}.
Despite their centrality to cognitive development and everyday conversation, a formal account of generic meaning remains elusive.

At first glance, generics seem like universally-quantified statements (i.e. ``All apples are red.''), but unlike universals, generics are resilient to counter-examples like \emph{Granny Smith apples}. 
Interpreting the generic as meaning ``most'' (i.e. ``Most apples are red.'') captures many cases, but cannot explain why ``Lions have manes'' and ``Mosquitos carry malaria.'' both seem true, despite the fact that among lions, only the adult males have manes and that a very small percentage of mosquitos actually carry malaria.
The only quantifier that could satisfy all of these conditions is the weak ``some'' (i.e. ``Some apples are red.'').  
Yet, the semantics for ``some'' is not the semantics of generics:  Upon hearing a generic, listeners are wont to interpret the sentence as applying to \emph{nearly all} of a category \cite{Gelman2002, Cimpian2010}. 
``Some apples are red.'' does not carry the same generalizability as ``Apples are red.''.
Additionally puzzling is the fact that while ``Lions have manes.'' is a fine generic utterance, ``Lions are male.'' is not, even though the prevalences of the two properties are the same for the category of lions.

%The semantics of generics is often contrasted with that of quantifier sentences, which are classically understood in terms of the prevalence of a property and are straightforward to evaluate for truth. 
%For example, ``\emph{Some} shoes have arch support.'' is true because there are shoes that do. 
%``\emph{Most} shoes have arch support.'' is probably true because probably more than half of shoes do have support. 
%``\emph{All} shoes have arch support.'' is definitely not true because of the existence of Vibram 5 toes, flat sandals, and many others.
%But what about ``Shoes have arch support.''? 
%Is there some prevalence beyond which the generic becomes true?
%Prevalence based accounts of generic meaning are difficult to defend. 

\subsubsection{The semantics and pragmatics of generic language  (400 words to this point)}

What could be the stable meaning of a generic given this extreme flexibility? 
We propose these phenomena can be explained as the effects of pragmatic inference filling in a meaning that is underspecified in the language. 
Pragmatic reasoning in communication has recently been formalized in probabilisitic models of language understanding \cite{Frank2012, Goodman2013, Franke2009} and explain how assumptions of truthful and informative interlocutors \cite{Clark1996, Grice1975, Levinson2000} produce rich meanings of linguistic expressions in context. 
We build upon this tradition to explain the semantics and pragmatics of generic language: We posit that generics are a vague description of prevalence in much the same way that gradable adjectives like \emph{tall} are vague descriptions of height. 
To understand the vagueness of ``tall'', consider the criteria by which a person is ``tall'' (e.g. \emph{$\sim$ 6 feet}) versus the conditions for a building to qualify as ``tall'' (e.g. \emph{$\sim$ 1000 feet}).
This context-sensitivity of meaning can be accounted for by positing the semantics of the word (``tall'') is a distribution over criteria (``anything could count as tall given the right context''). 
Probabilistic pragmatic approaches have formalized how this uncertainty is resolved by a listener consulting her beliefs about the domain in question (``what are probable heights of a person?'') as well as taking into account the communicative force of a speech act (``why did the speaker bother to say ``John is tall'' when he could have not said anything?'') \cite{Lassiter2013, Qing2014}.

%listeners arrive at rich interpretations of utterances by considering the thought-processes of a speaker whose goal is to be informative.
%The has provided formal, computational accounts of understanding utterances with quantifiers \cite{Goodman2013}, gradable adjectives \cite{Lassiter2015}, and nonliteral usages \cite{Kao2014}.
% for by treating the condition for tallness (i.e. the threshold beyond which something counts as tall) as an unknown property of the language, and modeling the listener as inferring this threshold in context \cite{Lassiter2015}. 
%The fact that the distributions of heights for buildings and people are different, and that listeners expect speakers to be for vague adjectives.
%
%
%The meanings of words are often embedded in a conversational context.
%Language understanding is a special case of social cognition---interlocutors think about one another when producing and interpreting utterances.  
%
%Recently, the Rational Speech-Act theory has been extended to account for gradable adjectives like \emph{tall}, which lack single, context-invariant meanings. 
%
%
%We propose that generics are vague in the same way that gradable adjectives, like \emph{tall}, are vague. 

We propose the literal semantics of a generic sentence ``K has F'' (e.g. ``Lions have manes.'') is a standard truth-functional, threshold meaning such that a category in question K has property F if the prevalence $x$ of F within K is greater than some threshold $\theta_{generic}$.

\begin{flalign}
\denote{\text{K has F}} = \{x^{\text{F}}_{K} : x^{\text{F}}_{K} = p(\text{F} | \text{K}) > \theta_{generic}\} \label{eq:literalgeneric}
\end{flalign}

The threshold $\theta_{generic}$, however, is left underspecified in the semantics.
Listeners have uncertainty about the threshold $\theta_{generic}$ \emph{a priori} and actively must reason about it in context to derive a realistic but informative meaning. 

The probabilistic model for generic interpretation, with an uncertain prevalence threshold is specified by:
%
\begin{flalign}
& P_{L_{1}}(x , \theta \mid g) \propto P_{S_{1}}(g \mid x, \theta) \cdot P(x) \cdot P(\theta) \label{eq:L1}
\end{flalign}
%
Eq.~\ref{eq:L1} is a model of a listener ($L_{1}$) who has been told a generic statement $g$ (e.g. ``Lions have manes.''). She has uncertainty about the prevalence of the property $x$ (``how many lions have manes?'') as well as the meaning of the generic $\theta$ (``what does this sentence even mean?''), and is trying to figure out what these are. 
$P(\theta)$ represents the listener's \emph{a priori} beliefs about the semantic variable $\theta$; we assume no informative beliefs about the threshold (i.e. $\theta \sim \text{Uniform}(0,1)$).
The listener resolves her uncertainty, in context, by consulting her prior beliefs about the property $P(x)$ and assuming that utterance was produced by a speaker $S_{1}$, whose goal was to be informative about the prevalence of the property $x$:
%
\begin{flalign}
& P_{S_{1}}(g \mid x, \theta) \propto \exp(\lambda \ln {P_{L_{0}}(x \mid g, \theta)}) \label{eq:S1}
\end{flalign}
%
The speaker knows the prevalence $x$ (e.g. knows that adult, male lions have manes). 
The listener ($L_{1}$) believes the speaker ($S_{1}$) to actually know the threshold $\theta$, and be a soft-max optimally informative speaker to a degree governed by $\lambda$ \cite{Luce1959}. 
This intuitive theory of a speaker is used to resolve this threshold and the prevalence $x$ by assuming the speaker was trying to be informative to a hypothetical literal listener:
%
\begin{flalign}
& P_{L_{0}}(x \mid g, \theta) \propto {\delta_{\denote{g(\theta)}(x)} P(x)} \label{eq:L0}
\end{flalign}
%
This idealized listener ($L_0$) has access to the threshold $\theta$ and thus knows exactly what the speaker meant to communicate. 
The literal listener simply uses Bayes' Rule where the likelihood function $\denote{g(\theta)}: X \rightarrow \text{Boolean}$ is a truth-function specifying the literal meaning of the generic given a threshold $\theta$, as in Eq.~\ref{eq:literalgeneric}. 
%The literal content in Eq.~\ref{eq:L0} is given by $\denote{g(\theta)}= \{x | x > \theta \}$ as in Eq.~\ref{eq:birds}.

This is a ``lifted variable'' pragmatics model because $\theta$, traditionally thought to be part of the semantic content of the utterance (and thus perfectly transparent to all in the conversation), has been underspecified in the semantics but is locally resolved through pragmatic reasoning.
How does a listener arrive upon a threshold? 
\citeA{Lassiter2013} demonstrated the inference about the threshold is a balance between truthfulness and informativity. 
Consider again the example of ``John is tall''. 
To make ``tall'' maximally truthful, the listener would set $\theta$ to be very low (e.g. 2 feet tall); no matter what John's height actually is, the utterance will be true since the threshold for tallness is so low. Here, truthfulness would be maximized. 
 Truthfulness is not the only constraint in conversation however; there is also the pressure to be informative. 
 A maximally informative ``tall'' would correspond to a threshold that carries with it the maximum surprisal or information gain on the part of the listener. 
 This would be a threshold that very few individuals could cross (e.g. 7 feet tall); this would correspond to something like ``the tallest''. 
 The lifted-variable pragmatics model seeks a balance between these two pressures. 
 The result is a threshold that signifies that ``John is tall'' means he is ``significantly taller than average''.
 
Eq.~\ref{eq:L1} is a model of generic interpretation: Upon hearing a generic, what prevalence is a listener likely to infer?'
A generic production rule would then naturally be specified by a speaker who has this sort of listener in mind:
%To explain the different conditions under which a generic utterance can be true, we extend the lifted-threshold model of \citeA{Lassiter2013} to include a model of a speaker who is deciding whether or not to produce the utterance. 
\begin{equation} 
P_{S_{2}}(g \mid x) \propto  \sum_{\theta} P_{L_{1}}(x , \theta \mid g) =  P_{L_{1}}(x \mid g)
\label{eq:S2}
\end{equation}
%
The speaker in \eqref{eq:S2} considers the thought-processes of the listener in Eq.~\eqref{eq:L1}, and decides if the generic $g$ is a good way to vaguely describe the prevalence $x$. 
This decision is with respect to one or more alternative speech-acts, which here we take to be the act of saying nothing\footnote{The results are equivalent both when the alternative is stipulated to be (1) the negation:  ````Lions have manes'' is false'' as well as (2) the generic of the negation i.e. ``Lions do not have manes''. These alternatives are analogous to alternatives to ``John is tall'' of ``John is not tall'' and ``John is short'' in the vague adjective case.}. 
Importantly, $S_{2}$ doesn't actually have access to the threshold $\theta$, but knows that $L_{1}$ is thinking about it, and marginalizes over possible inferred values. 
Eq.~\eqref{eq:S2} is a model of generic production: For a given prevalence, is the generic a good thing to say?

\subsubsection{Generics have flexible truth conditions (1400 words to this point)} 

We tested the degree to which Eq.~\eqref{eq:S2} predicted that a given category--prevalence pair (e.g. ``Lions'' and ``have manes'') would result in a good generic (e.g. ``Lions have manes.''). 
 The probabilisitic model is fully specified by Eqs.~\eqref{eq:literalgeneric} -- \eqref{eq:S2}, with the exception of the speaker rationality parameter $\lambda$ and the prior distribution on prevalences $P(x)$. 
 We use Bayesian data analytic techniques to integrate out $\lambda$, which is not of theoretical interest here. 
 $P(x)$ describes the belief distribution on the prevalence of a given property (e.g. ``have manes''). 
 This distribution is conceivably different for different types of properties. 
 Thus, we measure it empirically by asking participants ($n=30$) about the prevalence of different types of properties for many different animal categories\footnote{Our experiments stay within the animal kingdom because we expect there to be considerably less variability in participants beliefs about animals than about other types of categories (e.g. social categories)}. 
 
We analyzed responses for each property using the Bayesian data analytic approach described in Section \ref{sec:bda1}.	
The most likely prevalence priors inferred from the data are shown in Figure \ref{fig:priors1a} for 4 example properties.

\begin{figure}
\centering
    \includegraphics[width=0.8\columnwidth]{prevalence_priors_inferred-betas.pdf}
    \caption{Four example elicited prevalence priors. 
    Dotted lines show density by removing the mass at zero (corresponding to within-category prevalence). 
    For display purposes, this is omitted for ``have wings'', for which the density is off the scale and all at 1.
    Density plots reveal qualitatively different distributions for different properties. 
    Each sample from these distributions can be thought of to correspond to a particular animal category, representing the prevalence of the property within that animal category.
    Note the scale of the y-axis differs for each panel.}
  \label{fig:priors1a}
\end{figure}

There are substantial differences among the priors over prevalence between different types of properties. 
The distribution of ``has manes'' has the feature of being relatively rare across categories (large density at 0), and within a category there is considerable uncertainty in judgments as to what percentage of an arbitrary animal kind are expected have manes, though the expected value is around 50\% (Figure \ref{fig:priors1a} bottom left; within-category prevalence given by blue dotted line). 
By contrast, the distribution over ``is male'' has a lot less uncertainty: the property is highly prevalent across categories (almost no mass at 0), and within a category it is present in about 50\% of cases.
The property ``has wings'' is not particularly rare across categories (probably owing to the fact that bird categories are heavily represented in our stimulus set), and within a category is widespread, as reflected by the bimodal distribution. 
Finally, ``has malaria'' is an example of a property that is both rare across-species and within-species. 
The property is absent from most animal species, and even when it is present, it is only present in a small number of cases.

From this data, we can also estimate participants' beliefs about the prevalence of a property for a given category (e.g. the percentage of lions that have manes). The Maximum A-Posteriori (MAP) estimates and 95\% Bayesian Highest Density Intervals (HDI) over the mean prevalence for each property--category pairing of interest can be seen in Table \ref{tab:expt1}. 

With $P(x)$ empirically measured, the probabilistic model is fully specified. 
We compare the predictions of Eq.~\ref{eq:S2} to human judgments ($n=100$) about the acceptability of thirty generic sentences. 
We chose the sentences to cover a range of conceptual distinctions outlined by \citeA{Prasada2013}, including characteristic (e.g. ``Ducks have wings.''), minority (e.g. ``Lions have manes.''), striking (e.g. ``Mosquitos carry malaria.''), false generalization (e.g. ``Lions are male.''), and false (e.g. ``Lions lay eggs.'') bare plurals.

The 30 generic sentences fell into 3 \emph{a priori} categories: definitely true, definitely false, and neither true nor false (Figure \ref{fig:modeldataBars}, light bars). 
This \emph{a priori} distinction was a significant predictor of the eventual truth judgments: true generics were significantly more likely to be agreed with than the indeterminate generics ($\beta = 3.14; SE = 0.15; z = -20.9$), as revealed by a mixed-effect logistic regression with random by-participant effects of intercept.
Indeterminate generics were agreed with \emph{less} likely than chance ($\beta = -0.49; SE = 0.09; z = -5.3$) but significantly more than false generics ($\beta = 2.07; SE = 0.15; z = 14.1$).

About half of the variance in truth judgments are explained the prevalence of the property for the target category alone ($r^2 = 0.527$). 
This is, of course, expected given the existence of high-prevalence true generics (e.g. ``Leopards have spots.'') and low-prevalence false generics (e.g. ``Leopards have wings.''). 
However, large deviations from a purely within-category prevalence account remain: Generics with intermediate prevalences (prevalence quartiles 2 and 3: $ 22\% < prevalence < 62\%$), were not explained at all by prevalence ($r_{Q2,3}^2 = 0.006$).

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{tj_n100-postPred-byItem.pdf}
    \caption{Truth judgments from for thirty generic utterances and the predictive distribution for the lifted-variable model using empirical elicited priors. Order of the Y-axis corresponds to the rank-ordered model predictions. Error bars correspond with 95\% bootstrapped confidence intervals for the participant data and 95\% highest probability intervals for the model predictions.}
  \label{fig:modeldataBars}
\end{figure}

%\begin{figure}
%\centering
%    \includegraphics[width=\columnwidth]{tj_n100_tjVsPostpred_95hdi.pdf}
%    \caption{Truth judgments from Expt.~1b for each item vs. the posterior predictive MAP estimates for the target item using the lifted-threshold model with the empirical priors elicited in Expt.~1a. Color spectrum corresponds to the rank ordering of the truth judgment data; similarly colored dots received similar truth judgments. Error bars correspond with 95\% bootstrapped confidence intervals for the participant data and 95\% highest probability intervals for the model predictions.}
%  \label{fig:modeldataScatter}
%\end{figure}

The probabilisitic pragmatics model does a much better job of explaining the truth judgments ($r^2=0.974$). 
Generics that received definitive agreement or disagreement are predicted to be judged as such by the model (top and bottom portions of Figure \ref{fig:modeldataBars}). Unlike the model based on within-category prevalence alone, the lifted-threshold model does not break down for generics of categories with intermediate prevalence of the property (for prevalence quartiles 2 and 3, $r_{Q2,3}^2=0.975$. 

\subsubsection{Generics have strong implications (2500 words to this point)} 

Generic language exhibits mysterious behavior not only in how it is produced but in how it is understood.
Generics ``once accepted psychologically, ... appear to be commonly taken in a rather strong sense, as if the qualifier \emph{always} had implicitly crept into their interpretation'' \cite{Abelson1966}. \red{[Note: this is the same quote that Cimpian2010 use... is this okay?}.
%
\citeA{Gelman2002} found that adults interpret novel generic statements about familiar kinds (e.g. ``Bears like to eat ants.'') as implying that almost all of the category have the property (e.g. almost all bears like to eat ants).
%\citeA{Cimpian2010} found similar inferences with generics about unfamiliar kinds (e.g. ``Lorches have purple feathers.''). 

Interpretations of generic language are readily explored in our model of a listener (Eq.~\ref{eq:L1}) who hears a generic and is trying to infer what the world is like. 
Again, $P(x)$ was measured empirically.
Here, following \citeA{Cimpian2010}, we use novel animal categories and property (e.g. ``Lorches'' and ``has purple feathers'') to explore how beliefs are updated by generic language. 
It is pragmatically awkward to ask about participants beliefs about these novel category -- property pairs directly.
Rather, we took advantage of the latent structure in the prior elicitation task (Expt.~1a) to ask participants ($n=40$) about the prevalence of a property (e.g. ``has purple feathers'') within-categories and across-categories, separately. 
We used a hierarchical Bayesian approach to reconstruct the prevalence priors of a similar form to Expt.~1a (see Section \ref{sec:bda2} for details).

We used 4 different types of properties: body parts (e.g. ``has claws'') , colored body parts (e.g. ``has purple feathers''), biological adjectives (e.g. ``has small wings''), and common accidental or disease states (e.g. ``has wet fur'') and rare accidental or disease states (e.g. ``has broken legs'')\footnote{The distinction between common and rare accidental properties was determined empirically by oversampling those properties, analyzing the data first by item, and performing a median split based on the \emph{a priori} believed prevalence of the property}.

Figure \ref{fig:prior2} (left) shows the inferred distributions for prevalence $x$ for 5 different types of properties. 
Figure \ref{fig:prior2} (right) shows the region of interest of these distributions by removing the mass at 0. 
With the exception of the body part category, properties are mostly likely to be absent from the category (Figure \ref{fig:prior2} left; modes of distributions are at 0).
If the property is present in the category, the most likely prevalence for biological properties (``part'', ``color part'', and ``vague part'') is 100\% (Figure \ref{fig:prior2} right; modes of blue, green, and red distributions are at 1).
This is not the case with the prevalence priors for accidental properties, for which lower values are more likely (Figure \ref{fig:prior2} right; modes of orange and purples distributions are at some low prevalence).


\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{prior2_prevalenceprior-50k.pdf}
    \caption{Prevalence priors inferred from the prior elicitation experiment  (Expt.~1a). Right plot shows all non-zero probability mass. This corresponds to the expected within-category prevalence. Different types of properties have different likely prevalence values.}
  \label{fig:prior2}
\end{figure}

We compared the interpretations of the pragmatic listener in Eq.~\ref{eq:L1} to the participants judgments ($n=40$) about the likely prevalence of the property after hearing a generic (e.g. ``Lorches have purple feathers.''), in the same spirit as \citeA{Gelman2002, Cimpian2010}. Also following \citeA{Cimpian2010}, we recruited participants ($n=40$) to help determine the average prevalence at which a speaker would assent to the generic, which was previously found to be not as sensitive to participants beliefs about the property (but, cf. \citeA{Cimpian2010}, Expt.~4).

Implied prevalence of the generic was affected by the type of property in question (Figure \ref{fig:exp2b} Left; dark bars). 
%Of theoretical interest is whether or not the median split we performed on the accidental properties based on the results of the prior elicitation experiment resulted in different implications for the associated generic.
Interestingly, we observe greater implied prevalence of common accidental properties than rare accidental properties ($\beta=0.061; SE = 0.025; t(39) = 2.47; p = 0.018$, given our median split based on the prior elicitation task \footnote{These statistics are the result of a mixed-effects linear regression with a maximal mixed-effect structure: Random by-participant effects of intercept and slope}).
Implications of generics of body parts was significantly greater than those of the biological properties used by \citeA{Cimpian2010} (here, ``color parts'') ($\beta=0.118; SE = 0.024; t(39) = 4.74; p < 0.001$).
There was also a trending effect for the implications of vague body parts (e.g. curly fur) to be greater than those of color parts (e.g. yellow fur) ($\beta=0.032; SE = 0.016, t(54.8) = 1.95; p = 0.056$).
Also consistent with previous findings, there were no differences in the tendency to agree with the generic based on property type (Figure \ref{fig:exp2b} (Left; light bars)

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{asym-data-model-2opts-phi-100k.pdf}
    \caption{Generic utterances of unfamiliar categories truth conditions and implied prevalence (left) and model predictions (right). 
    This replicates and extends Cimpian et al. (2010). 
    Generic statements are accepted for a range of prevalences, resulting in an intermediate average prevalence (light bars) for ``truth conditions''. 
    Upon hearing a generic statement about biological properties, participants' infer that a high proportion of the category has the property (dark bars: red, blue and green). 
    Generics about accidental properties do not result in such a high implied prevalence (dark bars: purple and orange).  
    Error bars denote bootstrapped 95\% confidence intervals for the data and Bayesian 95\% credible intervals for the model.}
  \label{fig:exp2b}
\end{figure}


As for Expt.~1, we modeled the truth conditions task as a speaker (Eq.~\ref{eq:S2}) and submitted our model to the same analysis as the data (see Supplement). 
Of more interest here, generic interpretation is modeled as a listener (Eq.~\ref{eq:L1} and produces variable interpretations contingent on the type of property under discussion (Figure \ref{fig:exp2b}, Right, dark bars). These inferences align almost perfectly with participants interpretations of generic utterances ($r^2(10) = 0.97$). 


To understand the behavior of the model, consider again the beliefs about the properties inferred from Expt.~2a (Figure \ref{fig:prior2}). 
All of the properties have substantial mass at 0 (Figure \ref{fig:prior2} Left). 
This gives the speaker validity in saying the generic at low prevalence levels (though his confidence in doing so increases as prevalence increases).
The listener has the complementary task: She brings \emph{a priori} uncertainty about the generic threshold to the table.
For illustration, consider what would happen if she inferred the most conservative threshold (0) and responded with the \emph{maximum a posteriori} (MAP) of the distribution. 
A threshold of 0 produces Figure \ref{fig:prior2} (right), because it only rules out the possibility that 0\% of the category has the property. 
The resulting peaks (MAPs) of the distributions are near 1 for biological properties (parts, color parts, vague parts) and around 10\% for accidental properties (both rare and common). This alone would produce variable interpretations. 
Our listener, however, does something wiser: She integrates over her uncertainty about the threshold (believing the speaker to be both truthful and informative), and produces implications that both take into account the peaks and the shapes of the resulting distributions.
This results in subtle differences between the implications of body parts (e.g. ``Lorches have wings.'') and  color parts (e.g.``Lorches have purple wings.''). 







These strong inferences are puzzling when examined next to the prevalence required for a generic to be true. 
As we saw in Expt.~1, generics can be true for a range of prevalence levels. 
This phenomenon clearly distinguishes generic statements from quantified statements.
``All lorches have purple feathers.'' is true only when 100\% of lorches have purple feathers. 
Similarly, upon hearing such an utterance, one is likely to infer that 100\% of lorches have purple feathers.  
This symmetric relationship holds with the quantifier ``most'', but generic utterances don't behave in this way \cite{Cimpian2010}.
Generic statements are judged true for a wide range of prevalence levels, but upon hearing a generic utterance, participants were wont to infer that \emph{almost all} of the category have the property. 
The implications of generic statements go far beyond the evidence needed to accept them as valid utterances.

The strength of the inference also depends on the property in question.
\citeA{Cimpian2010} observed that predicating bare plurals with accidental properties (e.g. ``Lorches have muddy feathers.'') significantly reduced participants' judgments about the implied prevalence of these statements.

Expt.~2 attempted to replicate, extend and explain the findings of \citeA{Cimpian2010}: that there is an asymmetry between truth conditions and implied prevalence of the generic and that this asymmetry is sensitive to the type of property predicated. 
In Expt.~2a, we measure participants' beliefs about the distribution of these types of properties using a paradigm generalized from Expt.~1a. 
In Expt.~2b, we replicate and extend \citeauthor{Cimpian2010}'s findings to different classes of properties for which the prevalence priors differed. 
We show that the language understanding model predicts this asymmetry between truth conditions and implications, and that this asymmetry can be manipulated by differences in the prevalence distributions between properties.








The largest deviations occur for the items ``Robins carry malaria.'' and ``Sharks have manes.''. 
Neither of these is true, and participants judge them to be as such.
The model predictions these are bad but better than items such as ``Leopards have wings.'' 
This difference is likely driven by greater uncertainty about the prevalence of the properties ``carries malaria'' and ``has manes'' as compared with ``has wings''.
``Carries malaria'' and ``has manes'' produced a more uncertain distribution of responses \red{(cite concentration parameters here..)} compared with ``has wings.''








We use these estimates as the prevalence of property for the target category in the model. 





 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In the experiments that follow, we explore the predictions of the $S_2$ model by comparing them to participant acceptability judgments of natural cases of generic utterances. 
We also verify the predictions of the $L_1$ model by comparing them to participants' interpretations of novel generic sentences.
These predictions are borne out by first eliciting the prior distribution on prevalence.
 

In a similar way, we posit a simple, scalar semantics for generics in which they express that the probability of the property given the category-----i.e. the property's \emph{prevalence}-----is above a threshold (cf. \citeA{Cohen1999}). We treat this threshold as unknown property of the language and thus, as a variable that must be reasoned about in context. 

It might seem paradoxical that vague language should get so much usage. 
Shouldn't speakers want to express their ideas as clearly as possible?
Language evolutionary pressures suggest that, to the contrary, such underspecification is in fact useful, given that there is some context through which the uncertainty can be resolved \cite{Piantadosi2012}.
In this work, context takes the shape of a listener and speaker's shared beliefs (i.e. common ground) about the property in question. This, coupled with 
standard inferences from conversational pragmatics, allows the listener to figure out the meaning of the otherwise vague utterance.
We extend the \emph{Rational Speech-Acts} (RSA) framework \cite{Frank2012,Goodman2013} to consider how a speaker determines what generics are acceptable to produce and how a listener might interpret generics differently depending on her belief's about the property in question. 
This formalism gives a new, computational perspective on how ideas are conveyed and how beliefs play a central role in understanding language.

\subsection{The phenomena}

One important test for a theory of generic meaning is that it captures the intuition shared by language users about the truth of certain generic sentences. To test the predictions of this model, we must first measure participants' prior expectations about the prevalence of the properties in question. Critically, however, we must measure not only prior expectations about the prevalence of the property for the category \emph{alone} but the distribution of expectations across categories (i.e. not only the probability of laying eggs for a robin, but also the probability of laying eggs for a cow and other animal species).  
We show how prevalence within the category alone is insufficient to explain generic acceptability, but a model that reasons pragmatically about the distribution over prevalence is sufficient to explain the variability in truth judgments.

Generic statements are not only puzzling because of their variable truth conditions, but also because of how they are interpreted. \citeA{Gelman2002} found that listeners  interpret novel facts about animals in the form of generics (e.g. ``Bears like to eat ants.'') as applying to nearly all of the category. Interpretations of bare plurals of novel animals with accidental properties (e.g. ``Morseths have wet fur.'') were found to be much weaker \cite{Cimpian2010}. These differences in interpretation are thought to be a result of different beliefs about the properties in question. This intuition is readily formalized in our language understanding model. Beliefs about properties are measured in independent experiments to serve as a foundation for the model's predictions. 
We compare our model's predictions to experiments examining the acceptability of naturalistic generics and interpretations of generics about novel categories.
\section{Experiment 1: Truth value judgments}

The lifted-threshold RSA model specified in Eq.~\ref{eq:S2} predicts probabilities of producing a generic given a prior distribution on the prevalence of the property. 
In Expt.~1a, we measure the distribution on prevalence of certain target properties by asking participants about the percentage of a kind with the property \footnote{So as to not bias the measurement in favor of a small number of stipulated animal categories, we ask participants to generate their own animal categories.}. 
In Expt.~1b, we measure the acceptability of a number of generic statements about the properties measured in Expt. 1a. 
We show how the prevalence within a property alone is insufficient to explain the diversity of truth judgments of these generics, but how a model of a pragmatic speaker who considers the distribution on prevalence can explain this wide range of truth judgments.



\section{Experiment 2: Generics carry strong implications}
\subsection{Experiment 2a: \emph{Prior elicitation for properties of novel animals}}

In this experiment, we measured participants' beliefs about the distribution of the prevalence of certain properties within- and across- animal kinds. 
We expanded the stimulus set from \citeA{Cimpian2010} which consisted of novel animal categories (e.g. glippets) and various properties (e.g. have orange legs; have broken legs).


\subsubsection{Participants}

We recruited 40 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).  Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. All participants were native English speakers. The experiment took about 5-7 minutes and participants were compensated \$0.75.

\subsubsection{Procedure and materials}

In Expt.~1a, participants filled out a table with rows corresponding to different animal kinds and columns corresponding to different properties. 
Pilot testing suggested this was a pragmatically strange setup for this paradigm: answering ``What percentage of lorches have purple feathers?'', when participants knew nothing about lorches was difficult.
We took advantage of the latent structure in the task, described in Section \ref{sec:bda1}, and used a Bayesian statistical model to infer the underlying distribution. 
We then used these distributions in our language model to make predictions about the implications of generic sentences.

Participants were asked about the prevalence across categories by asking them the likelihood of there existing a K that has F, where K was a novel animal category and F was a property (e.g. ``how likely is it that there is a lorch that is female?''). 
Prevalence within categories was measured by asking participants about the percentage of Ks that have Fs, given that at least one does.
Participants responded using slider bars that ranged from ``unlikely'' to ``likely'', and ``0\%'' to ``100\%'', respectively.
Complete instructions shown to participants are in Appendix \ref{sec:prior2instruct}. 

Materials---novel animal names and familiar properties---built upon those from \citeA{Cimpian2010}. 
Classic work in generalization suggested to us that there may be differences in the implications of generic statements of different types of biological properties \cite{Nisbett1983}. 
We expanded the stimulus set to include four different types of properties: biological parts (e.g. ``feathers''), colored parts (e.g. ``purple feathers''), vague parts (e.g. ``smooth feathers''), and accidental parts (e.g. ``broken feathers''). 
Pilot testing revealed a lot of variability across items in the accidental properties relative to the other types of properties. 
To test the quantitative predictive power of the generic interpretation model, we used twice as many exemplars of accidental properties, with the aim to make a ``common accidental'' and a ``rare accidental'' class of properties. 
We used 8 exemplars of each of the three non-accidental properties (``parts'', ``colored parts'', ``vague parts'') and 16 exemplars of accidental properties.
Materials are in Appendix \ref{sec:materials2}.

\subsubsection{Bayesian data analysis}
\ref{sec:bda2}
In order to recover single belief distributions representing prevalence both within- and across- categories (analogous to those elicited in Expt.~1a and shown in Figure \ref{fig:priors1a}), we built a simple Bayesian statistical model of the task questions and their relation to the desired distribution. 

Participants' responses to each question (slider bar values) were assumed to be samples from Beta distributions with unknown means and concentrations. 
The responses to the questions ($d_{across}, d_{within}$) were used to estimate the parameters of the distributions of prevalence across- and within- categories, separately. 
\begin{align*}
d_{across} \sim \text{Beta}(\gamma_{across}, \delta_{across}) \\
d_{within} \sim \text{Beta}(\gamma_{within}, \delta_{within}) 
\end{align*}
The parameters $\gamma$ and $\delta$ correspond to the mean and variance (formally, the concentration), respectively, of prevalence for each of the across- and within- distributions.
As in our other models, we assume some global proportion of noise in the experimental data $\phi$. 
We put identical, uninformative priors on these parameters:
\begin{align*}
\phi & \sim \text{Uniform}(0,1) \\
\gamma & \sim \text{Uniform}(0,1) \\
\delta & \sim \text{Uniform}(0,20) 
\end{align*}
To construct single prevalence distributions reflecting both the within- and across- category prevalence (as we have for Expt.~1a), we assume that the distribution is a mixture of categories that have the property and categories that don't have the property\footnote{This is similar in spirit to Hurdle Models of count data used in clinical trials where the observed proportion of zeros is greater than one would expect from classical models of count data like the Poisson model.}. Whether or not a category has a property ($h$) is driven by the prevalence across categories ($\theta_{across}$).
%
\begin{align*}
\theta_{across} & \sim \text{Beta}(\gamma_{across}, \delta_{across}) \\ 
h & \sim \text{Bernoulli}(\theta_{across}) \\
x & \sim \begin{cases} 
		\text{Beta}(\gamma_{within}, \delta_{within}) &\mbox{if } h = 1 \\ 
				0 & \mbox{if } h=0. 
				\end{cases} 
\end{align*}
%
Marginal posterior distributions for $x$ were estimated using the Metropolis-Hastings algorithm in the probabilistic programming language WebPPL \cite{dippl}. Inference was completed by taking 50,000 samples using the Metropolis-Hastings algorithm.

\subsubsection{Results}

Our focus for this experiment is to see if the distributions on prevalence differ between the different types of properties.
We analyzed the data first by item in order to split the accidental properties into two categories: common accidental properties (e.g. wet fur) and rare accidental properties (e.g. broken legs). We then ran the model again analyzing the data by type of property.

Figure \ref{fig:prior2} (left) shows the posterior predictive distributions for prevalence $x$ for 5 different types of properties
.Figure \ref{fig:prior2} (right) shows the region of interest of these distributions by removing the mass at 0. 
These distributions form the prevalence priors used in the language understanding model for Expt.~2b. 
With the exception of the body part category, properties are mostly likely to be absent from the category (Figure \ref{fig:prior2} left; modes of distributions are at 0).
If the property is present in the category, the most likely prevalence for biological properties (``part'', ``color part'', and ``vague part'') is 100\% (Figure \ref{fig:prior2} right; modes of blue, green, and red distributions are at 1).
This is not the case with the prevalence priors for accidental properties, for which lower values are more likely (Figure \ref{fig:prior2} right; modes of orange and purples distributions are at some low prevalence).


\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{prior2_prevalenceprior-50k.pdf}
    \caption{Prevalence priors inferred from the prior elicitation experiment  (Expt.~1a). Right plot shows all non-zero probability mass. This corresponds to the expected within-category prevalence. Different types of properties have different likely prevalence values.}
  \label{fig:prior2}
\end{figure}

\subsection{Experiment 2b: mistmaches between \emph{truth conditions} and \emph{implications}}

\citeauthor{Cimpian2010} observed that generic statements of novel kinds with biological properties (e.g. ``Glippets have yellow fur'') show an asymmetry between the conditions by which the generic is true (``\emph{truth conditions}'') and the prevalence implied by the generic (``\emph{implied prevalence}''). 
Generic sentences were endorsed for a wide-range of prevalence levels (e.g. when ``30\% of glippets have yellow fur.''), resulting in intermediate average truth conditions. 
Upon hearing a generic, listeners inferred that the property was widespread (e.g. almost all glippets have yellow fur).
This mismatch between \emph{truth conditions} and \emph{implied prevalence} was significantly reduced for generics of properties plausibly construed as accidental (e.g. ``Glippets have wet fur.'').

In this experiment,  we replicate these findings and extend them to reveal even more variability in the mismatch between \emph{truth conditions} and \emph{implied prevalence} using the types of properties from Expt.~2a.
We also show how the prevalence-based model predicts the variability in the mismatch with strong quantitative accuracy.


\subsubsection{Participants}

We recruited 80 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).  
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. 
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

\subsubsection{Procedure and materials}

In order to get participants motivated to reason about novel animals, they were told they were the resident zoologist of a team of scientists that recently discovered an island with many new animals; their task was to provide their expert opinion on questions about these animals\footnote{The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}}. 
We recruited 40 participants for the \emph{truth conditions} task and 40 participants for the \emph{implied prevalence task}. 

Following \citeauthor{Cimpian2010}'s paradigm, in the \emph{truth conditions} task, participants were given an evidence statement consisting of the percentage of a novel animal category that had a property (e.g.~``30\% of glippets have yellow fur''). 
Participants were asked if they agreed or disagreed with the associated generic statement (i.e.~``Glippets have yellow fur.'').
Prevalence varied between 10, 30, 50, 70, and 90\%.
The experiment consisted of 25 trials: 5 trials for each of 5 types of properties measured in Expt.~2a (part, color part, vague part, common accidental, rare accidental). 
Each prevalence level appeared once for each property type (5 prevalence levels x 5 property types). 

Participants in the \emph{implied prevalence} task were supplied with the generic (e.g. ``Glippets have yellow fur.'') and asked to judge prevalence: ``What percentage of glippets do you think have yellow fur?''. Participants saw 25 trials: 5 for each of 5 property types.
The original study by \citeauthor{Cimpian2010} found a difference in the implied prevalence between ``color parts'' (e.g. yellow fur) and accidental properties (e.g. wet fur).
The prevalence priors inferred from Expt.~2a suggest that generic interpretation should be even more variable than simply strong vs. weak.
For this reason, we included three types of biological properties: parts (e.g. fur), color--part pairs (e.g. yellow fur) and gradable adjective--part pairs (e.g. curly fur). 
We also coded the accidental properties from Expt.~2a as either ``common'' or ``rare'' using a by-item median split.

 
Most of the materials we used were from \citeauthor{Cimpian2010}. 
The materials used were 30 novel animal categories (e.g. lorches, morseths, blins) each paired with a unique property. 
Biological properties were made by pairing a color with a body-part (e.g. purple feathers, orange tails). 
Accidental properties used the same set of body-parts but modified it with an adjective describing an accidental or disease state (e.g. broken legs, wet fur). 
Each participant saw a random subset of 10 unique animal-property pairs for each type of property (biological and accidental). 
Table \ref{tab:sampleTrial} shows an example trial for each of the property types and tasks.


\begin{table}[h]
\begin{tabular}{| l |  l | l | l |}
\hline
           &             & Truth conditions                                                                                                    & Implied prevalence                                            \\
           \hline \hline
Biological &             &                                                                                                                     &                                                               \\
\hline
           & Information & xx\% of lorches have purple feathers.                                                                               & Lorches have purple feathers.                                 \\
\hline
           & Question    & \begin{tabular}[c]{@{}l@{}}Is the following sentence true or false?\\ \\ Lorches have purple feathers.\end{tabular} & \begin{tabular}[c]{@{}l@{}}What percentage of lorches \\do you think have purple feathers?\end{tabular} \\
           \hline \hline
Accidental &             &                                                                                                                     &                                                               \\
\hline
           & Information & xx\% of lorches have muddy feathers.                                                                                & Lorches have muddy feathers.                                  \\
\hline
           & Question    & \begin{tabular}[c]{@{}l@{}}Is the following sentence true or false?\\ \\ Lorches have muddy feathers.\end{tabular}  & \begin{tabular}[c]{@{}l@{}}What percentage of lorches \\ do you think have muddy feathers?\end{tabular} \\
\hline
\end{tabular}
\caption{Sample item from Experiment 2}
\label{tab:sampleTrial}

\end{table}

\subsubsection{Results}



Results are shown in Figure~\ref{fig:exp2b} (Left). 
\subsubsection{Lifted-threshold model predictions}

In Expt.~1, the acceptability of the generic was modeled as a speaker (Eq.~\ref{eq:S2}) reasoning about whether or not to produce the generic given some known prevalence. 
We use the same model to make predictions for the \emph{truth conditions} data.
The \emph{implied prevalence} task is slightly different: Here, the participant hears a generic and is asked to infer the likely prevalence of the property. 
This is the model of the listener in Eq.~\ref{eq:L1}. 
We use this model to predict the data from the implied prevalence task.

Each of these models has a parameter governing the optimality of the hypothetical speaker in Eq.~\ref{eq:S1}. 
Since the supports of the distributions produced by the two models are different ($S_2$ returns a distribution over ``agree'' and ``disagree'', whereas $L_1$ generates a distribution prevalence levels), there is no reason to believe the speaker optimality parameters would be the same for the two models. 
Hence, we put independent prior distributions over the two parameters: $\lambda_{\text{truth conditions}}, \lambda_{\text{implied prevalence}} \sim \text{Uniform}(0, 20)$.

We model the observed data as being generated by a mixture of our generics model and a model of random guessing behavior. 
Modeling random guessing explicitly is important for recovering reliable estimates of the parameters of the model, which would otherwise be contaminated by this data \cite{LW2014}.
We put an uniformative prior over this mixture parameter $\phi \sim \text{Uniform}(0,1)$, and infer its credible values from the data.

\paragraph{Posteriors over model parameters}

To learn about the \emph{a posteriori} credible values of our model parameters, we used the probabilistic programming language WebPPL \cite{dippl} to collect 100,000 samples using the Metropolis-Hastings algorithm. 
The estimated posterior distribution of the contamination parameter $\phi$ parameter is shown in Figure \ref{fig:phi2}. 
This parameter represents the proportion of the data that is better explained by a model of random guessing than by the prevalence-based generics model. 
The 95\% Credible interval is [0.06, 0.15]. 

\begin{figure}
\centering
    \includegraphics[width=0.8\columnwidth]{asym-phi-2opts-phi-100k.pdf}
    \caption{Posterior distribution of the contamination (``guessing'') parameter. The 95\% credible interval is [0.06, 0.15].}
  \label{fig:phi2}
\end{figure}

The estimated posterior distribution of the speaker rationality parameters $\lambda_{\text{truth conditions}}$ and $\lambda_{\text{implied prevalence}}$ are shown in Figure \ref{fig:rationality2}. 
This parameter represents the belief in how rational the hypothetical speaker in Eq.~{ref:eqS1} is believed to be when choosing to say the generic (over saying nothing). 
The 95\% credible interval for $\lambda_{\text{truth conditions}}$ is [0.23, 6.9] and $\lambda_{\text{implied prevalence}}$ is [1.30, 16.86]. The fact that these parameters are so different is expected given that the state space in the two tasks is also quite different.


\begin{figure}
\centering
    \includegraphics[width=0.8\columnwidth]{asym-lambdas-2opts-phi-100k.pdf}
    \caption{Posterior distribution of the speaker rationality parameters. The 95\% credible interval for $\lambda_{\text{truth conditions}}$ is [0.23, 6.9] and  $\lambda_{\text{implied prevalence}}$ is [1.30, 16.86]. The fact that these parameters are so different is expected given that the state space in the two tasks is also quite different.}
  \label{fig:rationality2}
\end{figure}




\section{General discussion}

The lifted-threshold RSA model presented in this paper takes a generic statement to be vague. ``K has F'' means ``many members of K have F, \emph{relative to other categories, the vast majority of which, very few or none of the individuals in those categories have F}''. 
The model predicts a range of truth judgments for generics that seem to not have to do with prevalence. 
By considering not only the prevalence within the category but prevalence across categories, the model is able to arrive at graded and property-sensitive predictions about the truth judgments of generic statements, as seen in Expt.~1. 
The model very naturally accounts for the parallel problem of generic interpretation. 
Different \emph{a priori} beliefs about the distributions of properties both within- and across-categories give rise the puzzling asymmetries in how generics are interpreted relative to what is needed to felicitously use the generic. 

We present a context-invariant semantics for generic statements. 
The stable meaning of a generic is as simple as possible: a threshold on prevalence, where the threshold is uncertain \emph{a priori} and only resolved by pragmatic reasoning.
This simple semantics is consistent with the profound phenomenon from cognitive development: Generics are learned at a very young age \cite{Gelman1998, Gelman2004, Gelman2008, Cimpian2008}.

We have explained two outstanding puzzles in language understanding. 
The first is that generic utterances can be true for a wide range of prevalence levels: from tigers with stripes all the way down to mosquitos with West Nile Virus. 
How can the prevalence of the property alone explain what makes some generics good things to say and others not as good? 
The insight is that with an uncertain semantics, the likely meaning of the generic (i.e. the likely threshold for acceptance) is inferred using listeners' \emph{a priori} beliefs and the communicative force of a speech act. 
This leads to different thresholds for different types of properties. 

The second phenomenon is the generic utterances often carry strong implications, though not all bare plurals have this impact. 
We measured participants' beliefs about the distributions of properties and found the accidental properties are utterly distinct in shape from biological properties.
Biological properties are characterized by \emph{widespread prevalence}, even though they may be rare across kinds (e.g. purple feathers).
Accidental properties, by contrast, are both rare across and within kinds: \emph{broken legs} are \emph{a priori} most likely to present in a small fraction of the population.
Thus, when given a vague utterance in the form a generic, the most likely \emph{a posteriori} prevalences vary in ways that match human participants' intuitions about the implied prevalence.


\subsection{Conceptual distinctions and prevalence}

We have shown that the truth conditions and implications of generics can be explained by beliefs about the prevalence of properties within- and across- categories. 
However, there is an open question of where \emph{these distributions} come from. 
It is quite plausible that these distributions are derived from higher-order conceptual knowledge about the nature of these properties \cite{Gelman2005, Keil1992}.

In Expt.~1a, participants estimated the prevalence of properties for many different kinds of animals. This was aggregated to form a distribution of prevalence across kinds.
In Expt.~2a, we measured this distribution by sequentially asking questions at different levels of abstraction: Question 1 was about the prevalence across categories while Question 2 was about prevalence within categories. 
It is likely that further abstracting the problem to more conceptual level questions (e.g. ``Are there differences between male and female lorches?''; ``Do you think a young fep is likely to have long legs?'') would elucidate the connection between distributions on prevalence and conceptual representations. 

Much of the psychological and philosophical work has looked beyond prevalence and focused on conceptual distinctions among generics \cite{Prasada2013, Leslie2008}. For example, \citeauthor{Prasada2013} has argued for a distinction between \emph{characteristic} properties (e.g. ``Diapers are absorbent.'') and \emph{statistical} properties (e.g. ``Diapers are white.''). Where in the prevalence-based semantics could such conceptual distinctions come into play?

Probabilistic models are a useful way to represent rich, structured knowledge of world \cite{Goodmanconcepts}. It's plausible that the prevalence distribution we've focused on in this work is actually derived from richer conceptual knowledge. For the purpose of the semantics of generics, this work shows that prevalence is sufficient to capture the range of truth judgments for these sentences. How a listener arrives at an estimate of the prevalence, or the prevalence distribution at large, may be the result of a probabilistic, conceptual model of the world. 

It's important to stress that our approach puts at its core listeners' \emph{beliefs} about the prevalence of the properties in question. 
Our participants' estimated the prevalence of certain properties to be much higher than the actual statistics of the world \red(example?).
This is an important methodological contrast to what is often employed in semantic approaches to generics, that prevalence is about \emph{actual prevalence in the world}. 
Along similar lines, this work used utterances about animal kinds in these experiments because participants' beliefs about these properties and categories are likely to be relatively homogenous (thus, lower noise). However, generic language is often used in everyday conversation to talk about social categories: In these domains, we would expect large individual differences in generic acceptability and interpretations.


An additional issue, one that likely interacts with conceptual distinctions, is how to determine the kinds of things against which a speaker implicitly compares the prevalence of the category in question. Here, we have used only generic sentences about animals, where the likely contrast class is other animals (e.g. ``Mosquitos carry malaria, \emph{relative to other kinds of living creatures}''). Moving outside the domain of animals, the makeup of this contrast class becomes less clear. 


A further observation concerning the nature of this contrast class is that focus, as a result of prosodic cues for example, can change the distribution against which one compares the prevalence of a property. For example, is a person asks ``What carries malaria?'', a very natural answer would be ``MOSQUITOS carry malaria'', since Mosquitos more than any other animal kind carry malaria. By contrast, if a person asks ``What are mosquitos like?'', is it natural to reply ``Mosquitos CARRY MALARIA''? 
Our suspicion is that it is not as natural because of salient alternatives (``Mosquitos bite you'', ``Mosquitos suck blood.'', ``Mosquitos make buzzing sounds''). 
Here, the contrast class is not with respect to \emph{other animals} but with respect to other properties \emph{of that animal}. 
Notice, however, that the underlying dimension that these properties would be compared along is still prevalence. What has changed is what constitutes this distribution: prevalence across categories vs. prevalence across properties.

\subsection{Resilience to counter-examples}

Another main staple of generic utterances is that they are resilient to counter-examples. 
For example, observing a few (perhaps, well-behaved) dogs that do not bark does not falsify ``Dogs bark.'' 
If generic statements are indeed vague, this phenomenon is very similar to the Sorites' Paradox, wherein a vague utterance and a plausible inductive premise can lead to unintuitive conclusions.

To see the similarity, consider the Sorites Paradox for ``The Empire State Building is tall.'' The paradox comes out when the following argument is considered:

\begin{quotation}

The Empire State Building is tall.

Any building 0.5 meters shorter than a tall building is still tall.

Therefore, a single-story building is tall.

\end{quotation} 

The first premise is true with a high probability. 
The second premise, is true with high probability but becomes less plausible the shorter in height one's reference gets. 

For generics, the version of the Sorites would look like

\begin{quotation}
Dogs bark. 

If we observe one dog that doesn't bark, then we could still say ``Dogs bark''. 

Therefore, if there were no dogs that barked, we would still conclude ``Dogs bark''.
\end{quotation} 

One thing to notice is that generics' resilience to counter-examples bears a strong similarity to the traditional inductive premise of the Sorites. 
It may be that the this feature of generics falls out of the inherent uncertainty of the meaning (i.e. its vagueness).

It's also interesting to consider the conclusion of this generic Sorites. 
This takes a very similar form of classic thought-experiments in the linguistics literature (e.g. ``Even t.



\subsection{What are generics?}

Intuitions about what qualifies as a generic statement have led to the consensus that there are generic and non-generic meanings that can be derived from similar syntactic forms. For example,
\begin{quotation}
	(1) Tigers are massive. 
	
	(2) Tigers are on the front lawn.
\end{quotation}

Classically, (1) is understood as referring to the kind whereas (2) is understood as referring to a plurality of instances. 

In our follow-up to \citeauthor{Cimpian2010}'s study on the implied prevalence of generic statements about accidental properties, we found a gradient of implied prevalence, which was driven by listener's \emph{a priori} beliefs about the prevalence of the property. 
The finding that different properties receive variable interpretations suggests a categorical distinction between generic and non-generic bare plurals may not be necessary. 
Consider the following 

\begin{quotation}
	(3) Tigers are on front lawns. 
	
	(4) Tigers are in zoos.
	
	(5) Tigers are in open grasslands.
\end{quotation}

Many would argue that (3) is not generic. At the same time, it does seem to implicate more tigers than (2). The same can be observed by comparing (4) to (3). Indeed, (5) has the intuitive appeal of applying to a large chunk of the category. (5) could be interpreted generically. 

It seems that as you go from (3) - (5), the likely implied prevalence also increases, to the point where (5) might apply to category as a whole. 
With a generic whose meaning is vague, as we propose here, listeners need not make categorical distinctions to interpret a sentence. 
Rather, they only need consult their beliefs about the properties in question to resolve reasonable interpretations. 

\subsection{Vagueness and communication}

We have presented a semantics for generics in which they convey a threshold on prevalence which is uncertain but fixed through pragmatic reasoning about the distribution of prevalence for the property in question.
In Expt.~2a, we showed how this prevalence distribution can be elicited by sequentially asking about the prevalence across- and within- categories. 
Within-category prevalence was elicited by asking a question about generalization: ``Imagine there is a glippet that has green legs. What \% of glippets do you think have green legs?''.

The model of generic communication that we've introduced uses a threshold whose value is uncertain. 
In a sense, the bare minimum that the generic communicates is that we are in a situation where there is a glippet that has green legs. 
The listener then is then left to her own devices to generalize as widely as she deems based on her beliefs about the property. 
This is consistent with the notion of a GEN operator that tells the listener to generalize this fact \cite{Leslie2008}.


The generic, it seems, doesn't convey any additional information beyond what the listener already knew about the prevalence of the property.
This shouldn't surprise us. ``John is tall'' does not actually tell us about what \emph{tall} means\footnote{However, ``John is a person'' does tell a listener about what \emph{tall} means in ``John is tall''.}. 
Rather, the listener is expected to come to the conversation with some beliefs about heights, and knowing that John is a person, be able to infer likely meanings for \emph{tall}.


\subsection{Conclusion} 


We have explored and demonstrated the viability of a scalar semantics for generics when coupled with a sophisticated pragmatics. 
A lower-bound threshold on prevalence---the probability of the property given the category---is inferred as part of pragmatic interpretation, yielding vague and context sensitive meanings. 

We formalized reasoning about the threshold in a lifted-threshold Rational Speech Acts model. This model predicted graded truth judgements and an asymmetry between truth and prevalence judgments. It also naturally accommodates the role of context, explaining these effects as the result of variation in the prevalence prior. 

Generics are ubiquitous in natural language. It might seem paradoxical, then, that the semantics of generic statements are underspecified. Why should vague language get so much usage? One possibility is apparent in the lifted-variable RSA model: generic language provides interlocutors with the flexibility to convey meanings as rich as our conceptual knowledge. %, which are easily understood in context. 
Generics are vague, but predictable and useful.


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{generics}


\end{document}
